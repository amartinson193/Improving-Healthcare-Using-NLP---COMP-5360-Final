{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "built-romance",
   "metadata": {},
   "source": [
    "# Changes from v10\n",
    "* Using sentence tokenize as an initial pass in order to try and filter to the most pertinent information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-alabama",
   "metadata": {},
   "source": [
    "\n",
    "# Import the MIMIC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dominant-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (4,5,7,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "dataset_dictionary = {}\n",
    "\n",
    "for file_path in glob.glob('.\\\\Data\\\\MIMIC Files\\*'):\n",
    "    file_name = file_path.split('\\\\')[3].split('.')[0]\n",
    "    with gzip.open(file_path, mode='r') as file:\n",
    "        dataset_dictionary[file_name] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-desert",
   "metadata": {},
   "source": [
    "# Join the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "grateful-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset to join together -----\n",
    "\n",
    "# Create note_events table -----\n",
    "\n",
    "# Combine text for each subject and encounter\n",
    "note_events_base = dataset_dictionary['NOTEEVENTS'][dataset_dictionary['NOTEEVENTS'].loc[:,'CATEGORY'] == 'Discharge summary']\n",
    "note_events = note_events_base.groupby(['SUBJECT_ID', 'HADM_ID'], as_index=False)['TEXT'].agg(sum)\n",
    "\n",
    "# Create CPT table -----\n",
    "\n",
    "cpt_events_base = dataset_dictionary['CPTEVENTS']\n",
    "cpt_events_base = cpt_events_base[cpt_events_base['TICKET_ID_SEQ'] == 1]\n",
    "cpt_events_base = cpt_events_base.loc[:, ['SUBJECT_ID','HADM_ID', 'CPT_CD']]\n",
    "cpt_events = cpt_events_base.drop_duplicates()\n",
    "cpt_events\n",
    "\n",
    "# Join the datasets -----\n",
    "\n",
    "note_cpt = note_events.merge(cpt_events, on = ['SUBJECT_ID','HADM_ID'])\n",
    "# print(note_cpt.shape, note_events.shape, cpt_events.shape) # (223,150, 4) (52,726, 3) (227,510, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-dealing",
   "metadata": {},
   "source": [
    "# Filter the data to CPT over 200 samples + Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "pursuant-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99291    7860\n",
      "99223    2851\n",
      "99222    1736\n",
      "99254    1242\n",
      "99255     882\n",
      "         ... \n",
      "62272       1\n",
      "50547       1\n",
      "53215       1\n",
      "39561       1\n",
      "63082       1\n",
      "Name: CPT_CD, Length: 707, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Value Counts\n",
    "print(note_cpt['CPT_CD'].astype(str).value_counts())\n",
    "\n",
    "# Filter to CPT with over 200 notes\n",
    "df = note_cpt['CPT_CD'].astype(str).value_counts()\n",
    "top_200 = list((df[df > 200]).index.values)\n",
    "note_cpt_4 = note_cpt[note_cpt['CPT_CD'].astype(str).isin(top_200)]\n",
    "\n",
    "# Resample minority groups -----\n",
    "\n",
    "# Remove largest group\n",
    "top_200.remove('99291')\n",
    "'99291' in top_200\n",
    "\n",
    "# minority_ls = top_200\n",
    "minority_ls = ['99223','99222','99254']\n",
    "\n",
    "minority_df = []\n",
    "for i in minority_ls:\n",
    "    test_resampled = resample(note_cpt[note_cpt['CPT_CD'].astype(str) == i], replace=True, n_samples=7860, random_state=123)\n",
    "    minority_df.append(test_resampled)\n",
    "\n",
    "minority_df.append(note_cpt[note_cpt['CPT_CD'].astype(str) == '99291'])\n",
    "new_df = pd.concat(minority_df)\n",
    "\n",
    "new_df['CPT_CD'] = new_df['CPT_CD'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-steering",
   "metadata": {},
   "source": [
    "# Check for Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "altered-lucas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99291    7860\n",
       "99222    7860\n",
       "99254    7860\n",
       "99223    7860\n",
       "Name: CPT_CD, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(note_cpt['CPT_CD'].astype(str))\n",
    "# plt.show()\n",
    "\n",
    "new_df['CPT_CD'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-approach",
   "metadata": {},
   "source": [
    "# Sentence tokenizer to restructure dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "british-calcium",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-151-5e91ba314e0b>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[text_col_name] = dataframe[text_col_name].str.replace(',',' ')\n",
      "<ipython-input-151-5e91ba314e0b>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[text_col_name] = dataframe.apply(lambda row: sent_tokenize(row[text_col_name]), axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPT_CD</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99223</td>\n",
       "      <td>['Admission Date:  [**2155-4-15**]            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99223</td>\n",
       "      <td>['Admission Date:  [**2173-5-7**]             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99223</td>\n",
       "      <td>['Admission Date:  [**2176-9-22**]            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99223</td>\n",
       "      <td>['Admission Date:  [**2173-5-18**]            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99223</td>\n",
       "      <td>['Admission Date:  [**2116-5-14**]            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>99223</td>\n",
       "      <td>'Their phone number is\\n[**Telephone/Fax (1) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>99223</td>\n",
       "      <td>'Please call if you need to change your\\nappo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>99223</td>\n",
       "      <td>'??????'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>99223</td>\n",
       "      <td>'No follow up is needed with the liver doctors.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>99223</td>\n",
       "      <td>'Completed by:[**2173-5-29**]']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>571 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CPT_CD                                               TEXT\n",
       "0    99223  ['Admission Date:  [**2155-4-15**]            ...\n",
       "1    99223  ['Admission Date:  [**2173-5-7**]             ...\n",
       "2    99223  ['Admission Date:  [**2176-9-22**]            ...\n",
       "3    99223  ['Admission Date:  [**2173-5-18**]            ...\n",
       "4    99223  ['Admission Date:  [**2116-5-14**]            ...\n",
       "..     ...                                                ...\n",
       "728  99223   'Their phone number is\\n[**Telephone/Fax (1) ...\n",
       "733  99223   'Please call if you need to change your\\nappo...\n",
       "738  99223                                           '??????'\n",
       "743  99223   'No follow up is needed with the liver doctors.'\n",
       "748  99223                    'Completed by:[**2173-5-29**]']\n",
       "\n",
       "[571 rows x 2 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# pd.concat([test['column 1'].str.split(',', expand=True), test['predictive column']], axis=1).melt(id_vars='predictive column').drop('variable', axis=1)\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/33098040/how-to-use-word-tokenize-in-data-frame\n",
    "\n",
    "def expand_dataframe_by_sent(dataframe, text_col_name, pred_col_name):\n",
    "    \n",
    "    # # Step 1: Remove all commas\n",
    "    dataframe[text_col_name] = dataframe[text_col_name].str.replace(',',' ')\n",
    "\n",
    "    # # Step 2: Sentence tokenize and convert into a large string\n",
    "    dataframe[text_col_name] = dataframe.apply(lambda row: sent_tokenize(row[text_col_name]), axis=1)\n",
    "\n",
    "    # # Step 3 + 4: Split into columns & concatenate with original data\n",
    "    updated_df = pd.concat([dataframe[text_col_name].astype(str).str.split(',', expand=True), dataframe[pred_col_name]], axis=1).melt(id_vars=pred_col_name, value_name=text_col_name).drop('variable', axis=1)\n",
    "    \n",
    "    updated_df.dropna(inplace=True)\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "x = expand_dataframe_by_sent(new_df.iloc[0:5,:], 'TEXT', 'CPT_CD')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "painful-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.DataFrame.dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "expected-symposium",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-60f00bd4e9ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#     print('This is i',i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#     print(x['TEXT'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[1;34m'HI'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# test = note_cpt['TEXT'][9752]\n",
    "# test = pd.Series(test)\n",
    "\n",
    "# test = test.str.replace(':', ' ')\n",
    "# test.str.replace('s(?=\\s)', ' ', regex=True)[0]\n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# x = sent_tokenize(test.str.replace('s(?=\\s)', ' ', regex=True)[0])\n",
    "# for i in x:\n",
    "#     print(i)\n",
    "# pd.Series([x]).str.replace('\\[\\*\\*(.*?)\\*\\*\\]', ' ', regex=True)\n",
    "\n",
    "# new_df['']\n",
    "\n",
    "# for i, x in new_df.iloc[0:1,:].iterrows():\n",
    "#     print('This is i',i)\n",
    "#     print(x['TEXT'])\n",
    "['HI'].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-september",
   "metadata": {},
   "source": [
    "# Filter the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unable-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_data(text_series):\n",
    "    \n",
    "    # Replace \\n \n",
    "    text_series = text_series.str.replace('\\\\n',' ', regex=True)    \n",
    "\n",
    "    # Remove dates and locations\n",
    "    text_series = text_series.str.replace('\\[\\*\\*(.*?)\\*\\*\\]', ' ', regex=True)\n",
    "    \n",
    "    # Remove topics\n",
    "    data = text_series.str.split('([A-Z\\s]+:)')\n",
    "    for row_num, value in enumerate(data):\n",
    "        text_chunks = [x.strip().replace(':','').replace('\\n', '') for x in value]\n",
    "        for i, x in enumerate(text_chunks):\n",
    "            if 'MEDICATION' in x or 'SOCIAL HISTORY' in x or 'FAMILY HISTORY' in x:\n",
    "                text_chunks[i] = ' '\n",
    "                try:\n",
    "                    text_chunks[i + 1] = ' '\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        text_series.iloc[row_num] = ' '.join(text_chunks)\n",
    "    \n",
    "    # Replace punctuation\n",
    "    text_series = text_series.str.replace('[' + string.punctuation + ']', ' ', regex=True)\n",
    "    \n",
    "    # Convert to lowercase \n",
    "    text_series = text_series.str.lower()\n",
    "    \n",
    "    # Remove all digits\n",
    "    text_series = text_series.str.replace('\\d',' ', regex=True)\n",
    "    \n",
    "    # Replace plurals, endings with ing, endings with ed, endings with ly\n",
    "    text_series = text_series.str.replace('s(?=\\s)', ' ', regex=True)\n",
    "    text_series = text_series.str.replace('ing(?=\\s)', ' ', regex=True)\n",
    "    text_series = text_series.str.replace('ed(?=\\s)', ' ', regex=True)\n",
    "    text_series = text_series.str.replace('ly(?=\\s)', ' ', regex=True)\n",
    "    \n",
    "    return text_series\n",
    "\n",
    "# Update Text Column\n",
    "\n",
    "new_df.loc[:, 'TEXT'] = clean_data(new_df['TEXT']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "certified-meditation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admission date                 discharge date      date of birth                sex   m  service medicine  allergie  patient record  a  hav  no known allergie  to drug   attend   chief complaint jaundice  major surgical or invasive procedure   intubation  history of present illnes     yo m w  afib  htn p w jaundice and itch   symptom  start    wk  back  he notic  increas  itch  all over  yesterday he had a f u appt w  hi  card  dr   where they notic  that he wa  jaundic   he wa  sent to the   where abd u  show    cm pancreatic mas  and elevat  lft   tbili     with alk pho          pt also c o swell  in leg  for past   wk   he denie  pain in abd n v constipation abd distention  he ha  been suffer  from diarrhea for past   wk  and thought it to be d t lactyose intolerance  he say  it improv  after stopp  the milk product   he he lost    lb  and then gain    lb  over past   yr  the wt los  wa  intentional  hi  appetite i  fine  no insomnia  mood fine  denie  f c c  denie  cp  sob  dizzines   palpitation   he wa  admitt  to   on    he had a ct abd that show  a mark  bilary dilation without definitive mas  in the pancrea  seen  he wa  referr  for   on   but the procedure wa  not tolerat  well  he wa  brought back to   to be done under general anesthesia on    the procedure show  irregular malignant appear  high grade stricture in the distal cbd measur      cm  sphincterotomy and biliary stent placement were done  cytology brush   were taken  in the pacu he develop  acute shortnes  of breath and hypoxia  heart rate  were          per note   he had mild wheeze and bronchial breath sound   he wa  intubat  with etomidate succinylcholine  vital sign  post intubation were               ra  he dropp  hi  blood pressure requir  neosynephrine gtt     past medical history cad  mi in   echo show  old ipmi  stres  echo in   show  no e o ischemia  afib diagnos      underwent dccv in    now back in afib  htn obesity gout hypothyroidism shrapnel in hi  face dur  wwii   p removal   social history he i  a widow with two daughter     i  retir   quit smok     yr  back  smok  for   yr  quit etoh   yr  back  wa  a social drinker    fami  history no h o ca  cad  dm  physical exam                      ra f     nad heent icteru     no lad  mm chest ctabl heart rrr  no m r g  nl       abd soft  nt  obese  but no recent change per pt   b    extr    pitt  edema neuro no focal deficit  no asterixi    pertinent result  see attach  lab result  and   report  impression   normal major papilla   cannulation of the biliary duct wa  perform  with a sphincterotome us  a free hand technique    cholangiogram show  a irregular malignant appear  high grade stricture in the distal cbd measur      cm  the bile duct proximal to the stricture appear  dilat     a sphincterotomy wa  perform  in the    o clock position us  a sphincterotome over an exist  guidewire    cytology sample  were obtain  for histology us  a brush at the cbd stricture    a  cm by   f cotton   biliary stent wa  plac  successful  acros  the biliary stricture  micro stool  c diff negative x   all culture  without growth  cytology of biliary stricture atypical cell   brief hospital course   biliary obstruction  resolv  with   placement of stent  obstruction appear  malignant  concern  for possible pancreatic v cholangiocarcinoma   cytology inconclusive but ca      elevat    gi team i  concern  that pt would not like  tolerate more aggressive biopsy   they are consider  do  an endoscopic ultrasound with biopsy in the future and will take further brush   which they discus  with daughter       follow up   for biliary stent change in    schedul     hypoxic respiratory failure requir  intubation after    cause undetermin   no h o chf but some improvement with neb  and diurese   pt also had episode  of rapid af and so it wa  consider  that possib  thi  had caus  acute chf or aspiration give occurr  just after     continue neb   on corgard for many year  at home   cxr without pneumonia  did  have small b l pleural effusion   wa  in the icu  extubat         af pt ha  been on corgard for many year  which he ask  to continue rather than lopressor which wa  suggest    coumadin restart    at hi  home dose   thi  ne   to be held for one week prior to         hypotension   shock thi  occur  short  after   and resp failure requir  intubation   sepsi  wa  consider  given   manipulation   culture  neg  course of levofloxacin and flagyl       loose stool     set  c  diff toxin negative    medication  on admission cozaar   mg dai  nadolol   mg dai   wa  on lopressor in past but d c   a  fatigue wa  a e   aspirin   mg dai  warfarin  mg dai  lovastatin   mg dai  niaspan   tablet at bedtime triamterene mwf   discharge medication     heparin  porcine        unit ml solution sig             u injection tid    time  a day  until inr         ipratropium bromide        solution sig one     treatment inhalation q h  every   hour   a  need      warfarin   mg tablet sig one     tablet po dai     once dai  at     check dai  inr  adjust a  need      levofloxacin     mg tablet sig one     tablet po q  h  every    hour   for   day      metronidazole     mg tablet sig one     tablet po tid    time  a day  for   day      corgard    mg tablet sig one     tablet po once a day pt wa  on   mg per day at home  titrate up a  possible     cozaar    mg tablet sig one     tablet po once a day hold for sbp           aspirin    mg tablet sig one     tablet po once a day     niaspan     mg tablet sustain  release sig one     tablet sustain  release po at bedtime      triamterene    mg capsule sig one     capsule po every other day    discharge disposition extend  care  facility        discharge diagnosi  biliary obstruction  stricture on bile duct cad af htn hypothyroid   discharge condition stable   discharge instruction  please contact the doctor at the rehab with any worsen  abdominal pain  fever   or other concern  symptom    followup instruction  pt ne   to return to have hi  biliary stent replac  in    also  the pathology of the biliary stricture wa  inconclusive and so he may ne  further biopsy  thi  will be discuss  by dr    team with the fami   provider      md phone  date time      provider      st    gi room  date time      dr    office will contact your daughter  regard  additional follow up if need                                      md    complet  by']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(new_df.iloc[0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-bracket",
   "metadata": {},
   "source": [
    "# Shuffle the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "surprised-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.sample(n = len(new_df), random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-devil",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "extensive-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages -----\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "my_stop_words = list(set(stopwords.words('english'))) \\\n",
    "                + ['admission', 'date', 'sex'] \\\n",
    "                + ['needed', 'every', 'seen', 'weeks', 'please', 'ml', 'unit', 'small', 'year', 'old', 'cm', 'non', 'mm', 'however']\n",
    "                # Got the above from my top 100 most predictive words that I wanted to remove\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split the data -----\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_df['TEXT'].values, new_df['CPT_CD'].astype(str), test_size = .33, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-comment",
   "metadata": {},
   "source": [
    "# Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "determined-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data -----\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=my_stop_words, min_df = 3, max_df = .7, sublinear_tf=True)\n",
    "\n",
    "# Transform the training data\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-grenada",
   "metadata": {},
   "source": [
    "# Run Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cultural-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Naive Bayes model -----\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB(alpha=.7)\n",
    "\n",
    "# Fit and check accuracy\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "pred = nb_classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-decrease",
   "metadata": {},
   "source": [
    "# Tune NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sexual-mexico",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.421076621541547\n",
      "0.1\n",
      "0.421076621541547\n",
      "0.2\n",
      "0.421076621541547\n",
      "0.30000000000000004\n",
      "0.421076621541547\n",
      "0.4\n",
      "0.421076621541547\n",
      "0.5\n",
      "0.421076621541547\n",
      "0.6000000000000001\n",
      "0.421076621541547\n",
      "0.7000000000000001\n",
      "0.421076621541547\n",
      "0.8\n",
      "0.421076621541547\n",
      "0.9\n",
      "0.421076621541547\n",
      "1.0\n",
      "0.421076621541547\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def hyperparam_tuning(tfidf_train, y_train, tfidf_test, y_test, nb_classifier):\n",
    "    for i in np.arange(0,1.1,.1):\n",
    "        nb_classifier = MultinomialNB()\n",
    "        nb_classifier.fit(tfidf_train, y_train)\n",
    "        pred = nb_classifier.predict(tfidf_test)\n",
    "        print(i)\n",
    "        print(metrics.accuracy_score(y_test, pred))\n",
    "\n",
    "hyperparam_tuning(tfidf_train, y_train, tfidf_test, y_test, nb_classifier)  \n",
    "\n",
    "# Looks like .6-.7 are the best alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-guess",
   "metadata": {},
   "source": [
    "# Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "under-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_logist = LogisticRegression(C=.001, random_state = 42, multi_class = 'multinomial', penalty='l2')\n",
    "clf_logist.fit(tfidf_train, y_train)\n",
    "logist_pred = clf_logist.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-ultimate",
   "metadata": {},
   "source": [
    "# Looking at Feature Names and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "decent-lotus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33442"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notes\n",
    "# sum([np.exp(1)** x for x in nb_classifier.coef_[0]]) # The probability of all the words equals one\n",
    "# # Taken from here: * https://stackoverflow.com/questions/61586946/how-to-calculate-feature-log-prob-in-the-naive-bayes-multinomialnb\n",
    "\n",
    "# ------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_feature_rank(tfidf_vectorizer, y_no, nb_classifier):\n",
    "    \n",
    "    # Get the feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Zip together the first CPT weights with feature names\n",
    "    feat_with_weights =  sorted(zip(nb_classifier.coef_[y_no], feature_names))\n",
    "    \n",
    "    # Print words most responsible for the prediction\n",
    "#     print('Top 100 \\n\\n\\n\\n')\n",
    "#     top_100_ls = []\n",
    "    for i in range(100):\n",
    "        x = feat_with_weights[-i-1]\n",
    "#         top_100_ls.append(x[1])\n",
    "#         print(nb_classifier.classes_[y_no], i, round((np.exp(1) ** x[0]),4), x[1])\n",
    "\n",
    "#     print('\\n\\n\\n\\n Bottom 100 \\n\\n\\n\\n')\n",
    "    for i in range(100):\n",
    "        x = feat_with_weights[i]\n",
    "#         print(nb_classifier.classes_[y_no], i, round((np.exp(1) ** x[0]),4), x[1])\n",
    "    \n",
    "#     min_weight = min([i[0] for i in feat_with_weights])\n",
    "    \n",
    "    x = [i[0] for i in feat_with_weights]\n",
    "    \n",
    "    median_pred = np.median(x)\n",
    "          \n",
    "    return [i[1] for i in feat_with_weights if i[0] <= median_pred] # Minimum weight words\n",
    "#     return top_100_ls\n",
    "\n",
    "# Find the least predictive words\n",
    "def least_pred_words(nb_classifier, tfidf_vectorizer):\n",
    "    low_wt_stop_ls = []\n",
    "\n",
    "    for i in range(len(nb_classifier.classes_)):\n",
    "        low_wt_stop_ls += get_feature_rank(tfidf_vectorizer, i, nb_classifier)\n",
    "\n",
    "    low_wt_stop_ls = list(set(low_wt_stop_ls))\n",
    "    return low_wt_stop_ls\n",
    "    \n",
    "low_wt_stop_ls = least_pred_words(nb_classifier, tfidf_vectorizer)\n",
    "\n",
    "# Find top 100 words - doesn't seem to improve the model\n",
    "def highest_pred_words(nb_classifier, tfidf_vectorizer):\n",
    "    top_100_ls = []\n",
    "    for i in range(len(nb_classifier.classes_)):\n",
    "        top_100_ls += get_feature_rank(tfidf_vectorizer, i, nb_classifier)\n",
    "\n",
    "    top_100_ls = list(set(top_100_ls))\n",
    "    return top_100_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-summit",
   "metadata": {},
   "source": [
    "# Update stop words and tokenize again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "intense-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words += low_wt_stop_ls\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=my_stop_words, min_df = 3, max_df = .7, sublinear_tf=True)\n",
    "\n",
    "# Transform the training data\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-surface",
   "metadata": {},
   "source": [
    "# Create Vocab with top words and tokenize again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It reduced test accuracy back to 43% and training went from 50% to 44%\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=top_100_ls, stop_words=my_stop_words, min_df = 3, max_df = .7, sublinear_tf=True)\n",
    "\n",
    "# Transform the training data\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-remove",
   "metadata": {},
   "source": [
    "# Run Naive Bayes again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "painted-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Naive Bayes model -----\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB(alpha=.7)\n",
    "\n",
    "# Fit and check accuracy\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "pred = nb_classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-kenya",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "optimum-turkish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       99222       0.49      0.59      0.54      2541\n",
      "       99223       0.61      0.34      0.44      2611\n",
      "       99254       0.69      0.56      0.62      2606\n",
      "       99291       0.47      0.66      0.55      2618\n",
      "\n",
      "    accuracy                           0.54     10376\n",
      "   macro avg       0.56      0.54      0.54     10376\n",
      "weighted avg       0.56      0.54      0.54     10376\n",
      "\n",
      "Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       99222       0.53      0.62      0.57      5319\n",
      "       99223       0.68      0.41      0.51      5249\n",
      "       99254       0.72      0.61      0.66      5254\n",
      "       99291       0.51      0.70      0.59      5242\n",
      "\n",
      "    accuracy                           0.58     21064\n",
      "   macro avg       0.61      0.58      0.58     21064\n",
      "weighted avg       0.61      0.58      0.58     21064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create classification report taken from here: https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Test')\n",
    "class_labels = nb_classifier.classes_\n",
    "print(classification_report(y_test, pred,target_names=class_labels))\n",
    "\n",
    "print('Training')\n",
    "pred_x = nb_classifier.predict(tfidf_train)\n",
    "print(classification_report(y_train, pred_x,target_names=class_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "italian-serum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44368476229545556"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "# \"\"\"\n",
    "# V1 NLP Model Accuracy: 0.117\n",
    "# Wow, I've got a long way to go to improve accuracy\n",
    "# V2 NLP Model Accuracy: 0.14\n",
    "# V3 NLP Model Accuracy: .40\n",
    "# \"\"\"\n",
    "\n",
    "# Confusion matrix \n",
    "# confusion_mtrx = metrics.confusion_matrix(y_test.astype(str), pred) # 1380, 1380\n",
    "# confusion_mtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "quantitative-leadership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3560782168740599"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistical Model accuracy\n",
    "metrics.accuracy_score(y_test, logist_pred)\n",
    "# .39\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-wages",
   "metadata": {},
   "source": [
    "# Vectorize Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "objective-registrar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 2)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab = ['love', 'happy', 'run']\n",
    "count_vectorizer = CountVectorizer(vocabulary = vocab)\n",
    "x = count_vectorizer.fit_transform(['happy', 'run', 'run', 'run'])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-madness",
   "metadata": {},
   "source": [
    "# Splitting out a list in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "rotary-tumor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The other day</td>\n",
       "      <td>I saw a bear</td>\n",
       "      <td>A great big bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0              1                  2\n",
       "0  The other day   I saw a bear   A great big bear\n",
       "1            NaN            NaN                NaN\n",
       "2            NaN            NaN                NaN"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.DataFrame({'column 1':['The other day, I saw a bear, A great big bear',2,3], 'predictive column':[1,2,3]} )\n",
    "test['column 1'].str.split(',', expand=True)\n",
    "# pd.concat([test['column 1'].str.split(',', expand=True), test['predictive column']], axis=1).melt(id_vars='predictive column').drop('variable', axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
